{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1lwfeSJ3xcBu",
        "outputId": "22bcffcf-9b46-41ec-ee95-345591532571"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (6.0.2)\n",
            "Collecting python-dotenv>=1.0.0 (from -r requirements.txt (line 3))\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Collecting jupyter>=1.0.0 (from -r requirements.txt (line 5))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting notebook>=7.0.0 (from -r requirements.txt (line 6))\n",
            "  Downloading notebook-7.4.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.3.25)\n",
            "Collecting langchain-anthropic (from -r requirements.txt (line 8))\n",
            "  Downloading langchain_anthropic-0.3.14-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain_openai (from -r requirements.txt (line 9))\n",
            "  Downloading langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-community (from -r requirements.txt (line 10))\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 1)) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 5)) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter>=1.0.0->-r requirements.txt (line 5))\n",
            "  Downloading jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.11/dist-packages (from notebook>=7.0.0->-r requirements.txt (line 6)) (0.2.4)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=7.0.0->-r requirements.txt (line 6)) (6.4.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain->-r requirements.txt (line 7)) (0.3.60)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain->-r requirements.txt (line 7)) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->-r requirements.txt (line 7)) (0.3.42)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->-r requirements.txt (line 7)) (2.0.41)\n",
            "Collecting anthropic<1,>=0.52.0 (from langchain-anthropic->-r requirements.txt (line 8))\n",
            "  Downloading anthropic-0.52.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain->-r requirements.txt (line 7))\n",
            "  Downloading langchain_core-0.3.62-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai->-r requirements.txt (line 9)) (1.81.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai->-r requirements.txt (line 9)) (0.9.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r requirements.txt (line 10)) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->-r requirements.txt (line 10)) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->-r requirements.txt (line 10))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community->-r requirements.txt (line 10))\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community->-r requirements.txt (line 10))\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 11)) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 11)) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 10)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 10)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 10)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 10)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 10)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 10)) (1.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.52.0->langchain-anthropic->-r requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.52.0->langchain-anthropic->-r requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.52.0->langchain-anthropic->-r requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.52.0->langchain-anthropic->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.52.0->langchain-anthropic->-r requirements.txt (line 8)) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 10))\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 10))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (3.1.6)\n",
            "Collecting jupyter-client>=7.4.4 (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (5.7.2)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (5.10.4)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (0.22.0)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (24.0.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=5.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (5.7.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (1.8.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5))\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (7.34.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook>=7.0.0->-r requirements.txt (line 6)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->notebook>=7.0.0->-r requirements.txt (line 6)) (4.23.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 7)) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 7)) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 7)) (0.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.19.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->-r requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 9)) (2024.11.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.0.15)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 5)) (3.0.51)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic<1,>=0.52.0->langchain-anthropic->-r requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic<1,>=0.52.0->langchain-anthropic->-r requirements.txt (line 8)) (0.16.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (4.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=7.0.0->-r requirements.txt (line 6)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=7.0.0->-r requirements.txt (line 6)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=7.0.0->-r requirements.txt (line 6)) (0.25.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (4.3.8)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (2.21.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (0.7.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 10))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 5)) (2.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 5)) (0.8.4)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (24.11.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6)) (2.22)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=7.0.0->-r requirements.txt (line 6))\n",
            "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading notebook-7.4.3-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_anthropic-0.3.14-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_openai-0.3.18-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.52.1-py3-none-any.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.4.3-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.62-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.4/438.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, python-dotenv, overrides, mypy-extensions, marshmallow, json5, jedi, httpx-sse, fqdn, async-lru, typing-inspect, jupyter-server-terminals, jupyter-client, arrow, pydantic-settings, isoduration, dataclasses-json, anthropic, langchain-core, langchain_openai, langchain-anthropic, jupyter-events, langchain-community, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.60\n",
            "    Uninstalling langchain-core-0.3.60:\n",
            "      Successfully uninstalled langchain-core-0.3.60\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 6.5.7\n",
            "    Uninstalling notebook-6.5.7:\n",
            "      Successfully uninstalled notebook-6.5.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 7.4.3 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires notebook<7.0,>=5.7.6, but you have notebook 7.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anthropic-0.52.1 arrow-1.3.0 async-lru-2.0.5 dataclasses-json-0.6.7 fqdn-1.5.1 httpx-sse-0.4.0 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-8.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.3 jupyterlab-server-2.27.3 langchain-anthropic-0.3.14 langchain-community-0.3.24 langchain-core-0.3.62 langchain_openai-0.3.18 marshmallow-3.26.1 mypy-extensions-1.1.0 notebook-7.4.3 overrides-7.7.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 types-python-dateutil-2.9.0.20250516 typing-inspect-0.9.0 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "wJQQdGZNw28v"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import yaml\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from tqdm import tqdm\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain_core.exceptions import OutputParserException\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aCib4BYAqbN",
        "outputId": "777478b0-b851-451c-bba9-15df24ef6e32"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENDPOINT=https://iai-azoai-interview.openai.azure.com/\n",
            "DEPLOYMENT=your-deployment-id\n",
            "SUBSCRIPTION_KEY=FWKWFgUeps8e2QavlNhWHr8HVeiMmbfBinfyD8dxMY7lonPg5yQPJQQJ99BDACi0881XJ3w3AAABACOGOYou\n",
            "GITHUB_TOKEN=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"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Azure OpenAI config (optional, not used in basic scraping)\n",
        "endpoint = os.getenv(\"ENDPOINT_URL\")\n",
        "deployment = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "subscription_key = os.getenv(\"SUBSCRIPTION_KEY\")\n",
        "print(deployment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdWzoXRXxpL8",
        "outputId": "a63a8d9b-1bae-4117-ea03-90d7ff4fc1f5"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-4o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GitHub token for authenticated API access\n",
        "github_token = os.getenv('GITHUB_TOKEN')\n",
        "if not github_token:\n",
        "    raise ValueError(\"GITHUB_TOKEN not found in environment variables. Please check your .env file.\")"
      ],
      "metadata": {
        "id": "cRv1nfOQxqdt"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Settings ===\n",
        "USE_FILTERED_ACCOUNTS = True\n",
        "TARGET_REPOS = [\n",
        "    'alphagov', 'i-dot-ai', 'canada-ca', 'govtechsg', 'GSA', 'ec-europa', 'opengovsg'\n",
        "]"
      ],
      "metadata": {
        "id": "S4dGqn6fzAX6"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Functions ===\n",
        "\n",
        "def fetch_gov_github_accounts(url):\n",
        "    \"\"\"Fetch YAML list of GitHub accounts from governments.yml\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return yaml.safe_load(response.text)\n",
        "    else:\n",
        "        raise Exception(f\"Failed to fetch data: {response.status_code}\")\n",
        "\n",
        "def filter_repositories(accounts, target_repos):\n",
        "    \"\"\"Filter out only the GitHub usernames matching our target list\"\"\"\n",
        "    filtered_accounts = {}\n",
        "    for country, usernames in accounts.items():\n",
        "        filtered = [u for u in usernames if u in target_repos]\n",
        "        if filtered:\n",
        "            filtered_accounts[country] = filtered\n",
        "    return filtered_accounts\n",
        "\n",
        "def fetch_repository_details(username, token):\n",
        "    \"\"\"Fetch public repos for a GitHub user with optional README\"\"\"\n",
        "    headers = {'Authorization': f'token {token}'}\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
        "    repos_response = requests.get(repos_url, headers=headers)\n",
        "\n",
        "    if repos_response.status_code != 200:\n",
        "        print(f\"Failed to fetch repos for {username}: {repos_response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    repos_data = repos_response.json()\n",
        "    full_repo_details = []\n",
        "\n",
        "    for repo in repos_data:\n",
        "        details = {\n",
        "            'name': repo['name'],\n",
        "            'description': repo['description'] or \"No description\",\n",
        "            'stars': repo['stargazers_count'],\n",
        "            'forks': repo['forks'],\n",
        "            'language': repo['language'] or \"None specified\",\n",
        "            'readme': \"README not available\"\n",
        "        }\n",
        "\n",
        "        # Fetch README\n",
        "        readme_url = f\"https://api.github.com/repos/{username}/{repo['name']}/readme\"\n",
        "        readme_response = requests.get(readme_url, headers=headers)\n",
        "        if readme_response.status_code == 200:\n",
        "            readme_data = readme_response.json()\n",
        "            readme_raw = requests.get(readme_data['download_url']).text\n",
        "            details['readme'] = readme_raw[:300].replace('\\n', ' ')  # truncate and clean\n",
        "        full_repo_details.append(details)\n",
        "\n",
        "    return full_repo_details\n",
        "\n",
        "def save_to_markdown(repos, filename):\n",
        "    \"\"\"Write repo data to a Markdown table\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write('| Repository Name | Description | Stars | Forks | Language | README (truncated) |\\n')\n",
        "        f.write('|----------------|-------------|-------|-------|----------|---------------------|\\n')\n",
        "        for repo in repos:\n",
        "            f.write(f\"| {repo['name']} | {repo['description']} | {repo['stars']} | {repo['forks']} | {repo['language']} | {repo['readme'][:100]}... |\\n\")\n"
      ],
      "metadata": {
        "id": "KnG79eWgzN3d"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Main Execution ===\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://raw.githubusercontent.com/github/government.github.com/gh-pages/_data/governments.yml\"\n",
        "    all_accounts = fetch_gov_github_accounts(url)\n",
        "\n",
        "    accounts_to_use = filter_repositories(all_accounts, TARGET_REPOS) if USE_FILTERED_ACCOUNTS else all_accounts\n",
        "\n",
        "    all_repos = []\n",
        "\n",
        "    for country, usernames in accounts_to_use.items():\n",
        "        for username in tqdm(usernames, desc=f\"Processing {country}\"):\n",
        "            repos = fetch_repository_details(username, github_token)\n",
        "            all_repos.extend(repos)\n",
        "\n",
        "    # Save all to file\n",
        "    save_to_markdown(all_repos, \"gov_github_repos.md\")\n",
        "    print(\" Done. Output saved to gov_github_repos.md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUOyQ7ZmzV0l",
        "outputId": "ba9b5fd8-beaa-4c57-ec28-21c1b79c523e"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Canada: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch repos for canada-ca: 401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing European Union: 100%|██████████| 1/1 [00:00<00:00, 11.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch repos for ec-europa: 401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Singapore:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch repos for govtechsg: 401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Singapore: 100%|██████████| 2/2 [00:00<00:00, 13.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch repos for opengovsg: 401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing U.K. Central:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch repos for alphagov: 401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing U.K. Central: 100%|██████████| 2/2 [00:00<00:00, 12.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch repos for i-dot-ai: 401\n",
            " Done. Output saved to gov_github_repos.md\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second part of the analysis"
      ],
      "metadata": {
        "id": "Cq_Gz9s40U9v"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURE YOUR OPENAI API KEY\n",
        "openai.api_key = subscription_key"
      ],
      "metadata": {
        "id": "yPXXhqsG0dcL"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: README texts with associated country/org metadata\n",
        "data = [\n",
        "    {\"id\": 1, \"text\": \"This project is designed to simplify data workflows.\", \"country\": \"USA\", \"org\": \"OrgA\"},\n",
        "    {\"id\": 2, \"text\": \"Our library improves AI-driven content summarization.\", \"country\": \"Canada\", \"org\": \"OrgB\"},\n",
        "    {\"id\": 3, \"text\": \"Focuses on scalable trend analysis for multiple industries.\", \"country\": \"USA\", \"org\": \"OrgA\"},\n",
        "    {\"id\": 4, \"text\": \"Provides sentiment insights from customer feedback.\", \"country\": \"Germany\", \"org\": \"OrgC\"},\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "KdHB55mx1KpK"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "import json"
      ],
      "metadata": {
        "id": "r4J0Ohy67Kwq"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O937qaG8Zra",
        "outputId": "27cbe21f-82ca-41f6-b8bd-3a1d73d17e48"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENDPOINT_URL=https://iai-azoai-interview.openai.azure.com/\n",
            "DEPLOYMENT=your-deployment-id\n",
            "SUBSCRIPTION_KEY=FWKWFgUeps8e2QavlNhWHr8HVeiMmbfBinfyD8dxMY7lonPg5yQPJQQJ99BDACi0881XJ3w3AAABACOGOYou\n",
            "GITHUB_TOKEN=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"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from dotenv import load_dotenv\n",
        "#import os\n",
        "\n",
        "#load_dotenv()\n",
        "\n",
        "#endpoint = os.getenv(\"ENDPOINT\")\n",
        "#deployment = os.getenv(\"DEPLOYMENT\")\n",
        "#subscription_key = os.getenv(\"SUBSCRIPTION_KEY\")\n",
        "#endpoint = os.getenv(\"ENDPOINT\")\n",
        "#deployment = os.getenv(\"DEPLOYMENT\")\n",
        "#subscription_key = os.getenv(\"SUBSCRIPTION_KEY\")\n",
        "print(endpoint)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dof04svG74PG",
        "outputId": "9ef02e36-4a28-4488-e6d7-3a506d52b199"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://iai-azoai-interview.openai.azure.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LangChain AzureChatOpenAI client\n",
        "chat = AzureChatOpenAI(\n",
        "    deployment_name=deployment,\n",
        "    openai_api_base=endpoint,\n",
        "    openai_api_key=subscription_key,\n",
        "    openai_api_version=\"2025-01-01-preview\",\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "def analyze_text(text, task):\n",
        "    prompt_map = {\n",
        "        \"summarize\": f\"Please provide a concise summary of the following text:\\n{text}\",\n",
        "        \"sentiment\": f\"Please provide the sentiment (positive, neutral, negative) of the following text:\\n{text}\"\n",
        "    }\n",
        "    prompt = prompt_map.get(task)\n",
        "    if not prompt:\n",
        "        raise ValueError(f\"Unsupported task: {task}\")\n",
        "\n",
        "    response = chat([HumanMessage(content=prompt)])\n",
        "    return response.content.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I_aOGLb7OwY",
        "outputId": "161b9b84-6578-4e4f-e3b7-ff5ca038bcd0"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/azure_openai.py:174: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://iai-azoai-interview.openai.azure.com/ to https://iai-azoai-interview.openai.azure.com/openai.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/azure_openai.py:181: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/azure_openai.py:189: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://iai-azoai-interview.openai.azure.com/ to https://iai-azoai-interview.openai.azure.com/openai.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    {\"id\": 1, \"text\": \"This project is designed to simplify data workflows.\", \"country\": \"USA\", \"org\": \"OrgA\"},\n",
        "    {\"id\": 2, \"text\": \"Our library improves AI-driven content summarization.\", \"country\": \"Canada\", \"org\": \"OrgB\"},\n",
        "    {\"id\": 3, \"text\": \"Focuses on scalable trend analysis for multiple industries.\", \"country\": \"USA\", \"org\": \"OrgA\"},\n",
        "    {\"id\": 4, \"text\": \"Provides sentiment insights from customer feedback.\", \"country\": \"Germany\", \"org\": \"OrgC\"},\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "8-hh4TZ79YCF"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply summary and sentiment analysis\n",
        "df[\"summary\"] = df[\"text\"].apply(lambda x: analyze_text(x, task=\"summarize\"))\n",
        "df[\"sentiment\"] = df[\"text\"].apply(lambda x: analyze_text(x, task=\"sentiment\"))\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CIPg-M59fgK",
        "outputId": "e4b5ae3d-6f0d-4664-804e-81cbe9002a3f"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                               text  country   org  \\\n",
            "0   1  This project is designed to simplify data work...      USA  OrgA   \n",
            "1   2  Our library improves AI-driven content summari...   Canada  OrgB   \n",
            "2   3  Focuses on scalable trend analysis for multipl...      USA  OrgA   \n",
            "3   4  Provides sentiment insights from customer feed...  Germany  OrgC   \n",
            "\n",
            "                                             summary  \\\n",
            "0  The project aims to streamline and simplify da...   \n",
            "1  The library enhances AI-based content summariz...   \n",
            "2  The text highlights the emphasis on scalable t...   \n",
            "3  Analyzes customer feedback to determine sentim...   \n",
            "\n",
            "                                           sentiment  \n",
            "0         The sentiment of the text is **positive**.  \n",
            "1  The sentiment of the text is **positive**, as ...  \n",
            "2  The sentiment of the text is **neutral**. It i...  \n",
            "3  The sentiment of the text is **neutral**. It i...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count sentiment occurrences per country\n",
        "sentiment_by_country = df.groupby(['country', 'sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "# Count sentiment occurrences per organization\n",
        "sentiment_by_org = df.groupby(['org', 'sentiment']).size().unstack(fill_value=0)"
      ],
      "metadata": {
        "id": "8pjw2tcX-sYR"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save analyzed DataFrame with summaries and sentiments\n",
        "df.to_csv(\"readme_analysis.csv\", index=False)\n",
        "df.to_json(\"readme_analysis.json\", orient=\"records\", indent=2)\n",
        "\n",
        "# Save sentiment trends to CSV\n",
        "sentiment_by_country.to_csv(\"sentiment_by_country.csv\")\n",
        "sentiment_by_org.to_csv(\"sentiment_by_org.csv\")"
      ],
      "metadata": {
        "id": "pwRr_4f2-vJm"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#strategy_comment\n",
        "#The strategy for identifying opportunities involved analyzing README content from government-related GitHub repositories across multiple countries and organizations.\n",
        "#By applying content summarization, we distilled the core purpose of each project to understand focus areas.\n",
        "#Sentiment analysis of README text provided insight into the general tone or outlook of projects (positive, neutral, negative).\n",
        "#Aggregating these results by country and organization allowed us to identify geographical and institutional trends,\n",
        "#highlighting where innovation or concerns are more prominent.\n",
        "#This approach surfaces actionable insights for stakeholders interested in government tech and open source trends."
      ],
      "metadata": {
        "id": "Uh1n8_YD-0Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "#### Future Work\n",
        "####"
      ],
      "metadata": {
        "id": "SukdI_DdG4Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame (replace with your actual `df`)\n",
        "df = pd.DataFrame([\n",
        "    {\"id\": 1, \"text\": \"AskGov is a chatbot platform...\", \"summary\": \"AskGov is a chatbot Q&A for public services.\", \"sentiment\": \"Positive\", \"country\": \"Singapore\", \"org\": \"opengovsg\", \"name\": \"AskGov\", \"language\": \"TypeScript\", \"repo_url\": \"https://github.com/opengovsg/askgovsg\"},\n",
        "    {\"id\": 2, \"text\": \"This is a cloud platform...\", \"summary\": \"Enables cloud infrastructure for public orgs.\", \"sentiment\": \"Neutral\", \"country\": \"USA\", \"org\": \"GSA\", \"name\": \"cloud-gov\", \"language\": \"Go\", \"repo_url\": \"https://github.com/18F/cg-site\"},\n",
        "    {\"id\": 3, \"text\": \"Digital Service APIs for UK gov...\", \"summary\": \"Standard APIs for public services.\", \"sentiment\": \"Neutral\", \"country\": \"UK\", \"org\": \"alphagov\", \"name\": \"govuk-api\", \"language\": \"Ruby\", \"repo_url\": \"https://github.com/alphagov/govuk-api\"}\n",
        "])\n",
        "\n",
        "# Filter top candidates based on positive sentiment\n",
        "positive_projects = df[df[\"sentiment\"].str.lower() == \"positive\"]\n",
        "\n",
        "# Choose top candidate (you can use stars or priority logic if available)\n",
        "top_candidate = positive_projects.iloc[0]\n"
      ],
      "metadata": {
        "id": "pI-tL1f7G7N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the final recommendation\n",
        "recommendation = f\n",
        "print(\"INVESTMENT RECOMMENDATION REPORT\")\n",
        "\n",
        "print(\"TOP PROJECT:\")\n",
        "print(\"Name:\" + {top_candidate['name']})\n",
        "print(\"Organization:\" + {top_candidate['org']})\n",
        "print(\"Country:\" + {top_candidate['country']})\n",
        "print(\"Language:\" + {top_candidate['language']})\n",
        "print(\"Repository:\" + {top_candidate['repo_url']})\n",
        "\n",
        "print(\"Summary:\")\n",
        "print({top_candidate['summary']})\n",
        "\n",
        "#Strategic Justification:\n",
        "#- AI-driven public service chatbot\n",
        "#- Improves citizen engagement and accessibility\n",
        "#- Open-source and scalable across governments\n",
        "#- Strong sentiment from LLM analysis\n",
        "#- Based in Singapore, a leader in GovTech innovation\n",
        "\n",
        "print(\"Recommendation:\")\n",
        "#print(\"Prioritize pilot investment or partnership with {top_candidate['org']}` to adapt and deploy `{top_candidate['name']}` in other civic contexts (e.g. local governments, federal digital services).\")\n",
        "\n",
        "\n",
        "# Print or save to file\n",
        "print(recommendation)\n",
        "\n",
        "# Optional: save to a markdown or text file\n",
        "with open(\"investment_recommendation.txt\", \"w\") as f:\n",
        "    f.write(recommendation)"
      ],
      "metadata": {
        "id": "ZwNJ0JwjHDQh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}